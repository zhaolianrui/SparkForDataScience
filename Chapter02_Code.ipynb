{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Spark_Version:2.0.0<br/>\n",
    "Python_Version:Python 3.5.2 | Anaconda4.1.1(64-bit)<br/>\n",
    "Jupyter_Version:4.2.1</br>\n",
    "System:Ubuntu 16.04 LTS(64-bit)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark_Version: 2.0.0\n",
      "Python_Version: 3.5.2\n",
      "System: Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(\"Spark_Version:\",sc.version)\n",
    "print(\"Python_Version:\",platform.python_version())\n",
    "print(\"System:\",platform.system())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RDDs\n",
    "In this section, we will use both a Python shell (PySpark) and a Scala shell (Spark-Shell) to create an RDD. Both of these shells have a predefined, interpreter-aware SparkContext that is assigned to a variable sc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pass the file path to create an RDD \n",
    "#from the local file system\n",
    "fileRDD = sc.textFile('README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fileRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# SparkForDataScience'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#action method. Evaluates RDD DAG and also returns\n",
    "#the first item in the RDD along with the time taken\n",
    "fileRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed the whole cycle of initiating a Spark application (shell), creating an RDD, and consuming it. Since RDDs are recomputed every time an action is executed, fileRDD is not persisted in the memory or hard disk. This allows Spark to optimize the sequence of steps and execute intelligently. <strong>In fact, in the previous example, the optimizer would have just read one partition of the input file because first() does not require a complete file scan</strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example creates an RDD by passing a Python/Scala list with the parallelize function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD = sc.parallelize([1,2,3,4], 2)\n",
    "type(numRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD.map(lambda x: x*x).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD creation by passing in-memory collections is simple but may not work very well for large collections, <strong>because the input collection should fit completely in the driver node's memory</strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a Spark program usually consists of transformations and actions. Transformations are lazy operations that define how to build an RDD. Most of the transformations accept a single function argument. All these methods convert one data source to another. Every time you perform a transformation on any RDD, a new RDD will be generated, even if it is a small change,This is because the RDDs are immutable (read-only) abstractions by design. The resulting output from an action can either be written back to the storage system or it can be returned to the driver program for local computation if needed to produce the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The filter operation\n",
    "The `filter` operation returns an RDD with only those elements that satisfy a filter condition, similar to the `WHERE` condition in `SQL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5,6], 3)\n",
    "b = a.filter(lambda x: x % 3 == 0)\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The distinct operation\n",
    "The `distinct([numTasks])` operation returns an RDD with a new dataset after eliminating(消除) duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack', 'John', 'Mike']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = sc.parallelize(['John', 'Jack', 'Mike', 'Jack'], 2)\n",
    "c.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The intersection operation\n",
    "The intersection operation takes another dataset as input. It returns a dataset that contains common elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 9, 10, 6, 7]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "y = sc.parallelize([5,6,7,8,9,10,11,12,13,14,15])\n",
    "z = x.intersection(y)\n",
    "z.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The union operation\n",
    "The union operation takes another dataset as input. It returns a dataset that contains elements of itself and the input dataset supplied to it. If there are common values in both sets, then they will appear as duplicate values in the resulting set after union:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 7, 8, 9]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([3,4,5,6,7], 1)\n",
    "b = sc.parallelize([7,8,9], 1)\n",
    "c = a.union(b)\n",
    "c.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The map operation\n",
    "The map operation returns a distributed dataset formed by executing an input function on each of the elements in the input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('animal', 6), ('human', 5), ('bird', 4), ('rat', 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([\"animal\", \"human\", \"bird\", \"rat\"], 3)\n",
    "b = a.map(lambda x: len(x))\n",
    "c = a.zip(b)\n",
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animal', 'human', 'bird', 'rat']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The flatMap operation\n",
    "The flatMap operation is similar to the map operation.While map returns one element per input element, flatMap returns a list of zero or more elements for each input element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5], 4)\n",
    "a.flatMap(lambda x: range(1, x+1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 10, 10, 10, 20, 20, 20]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([5, 10, 20], 2).flatMap(lambda x: [x, x, x]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The keys operation\n",
    "The keys operation returns an RDD with the key of each tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 5, 5, 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize(['black', 'blue', 'white', 'green', 'grey'], 2)\n",
    "b = a.map(lambda x: (len(x), x))\n",
    "c = b.keys()\n",
    "c.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cartesian operation\n",
    "The cartesian operation takes another dataset as argument and returns the Cartesian product(笛卡儿积) of both datasets. This can be an expensive operation, returning a dataset of size m x n where m and n are the sizes of input datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10),\n",
       " (1, 11),\n",
       " (1, 12),\n",
       " (2, 10),\n",
       " (3, 10),\n",
       " (2, 11),\n",
       " (2, 12),\n",
       " (3, 11),\n",
       " (3, 12)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([1, 2, 3])\n",
    "y = sc.parallelize([10, 11, 12])\n",
    "x.cartesian(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on pair RDDs\n",
    "Some Spark operations are available only on RDDs of key value pairs. <strong>Note that most of these operations, except counting operations, usually involve shuffling, because the data related to a key may not always reside on a single partition</strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The groupByKey operation\n",
    "Similar to the SQL groupBy operation, this groups input data based on the key and you can use aggregateKey or reduceByKey to perform aggregate operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ['blue', 'grey']), (5, ['black', 'green', 'white'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([\"black\", \"blue\", \"white\", \"green\", \"grey\"], 2)\n",
    "b = a.groupBy(lambda x: len(x)).collect()\n",
    "sorted([(x, sorted(y)) for (x,y) in b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The join operation\n",
    "The join operation takes another dataset as input. Both datasets should be of the <strong>key value</strong> pairs type. The resulting dataset is yet another key value dataset having keys and values from both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'blue'), (5, 'green'), (6, 'orange')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([\"blue\", \"green\", \"orange\"], 3)\n",
    "b = a.keyBy(lambda x: len(x))\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 'black'), (5, 'white'), (4, 'grey')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = sc.parallelize(['black', 'white', 'grey'], 3)\n",
    "d = c.keyBy(lambda x: len(x))\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ('blue', 'grey')), (5, ('green', 'black')), (5, ('green', 'white'))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.join(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, ('orange', None)),\n",
       " (4, ('blue', 'grey')),\n",
       " (5, ('green', 'black')),\n",
       " (5, ('green', 'white'))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leftOuterJoin\n",
    "b.leftOuterJoin(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ('blue', 'grey')), (5, ('green', 'black')), (5, ('green', 'white'))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rightOuterJoin\n",
    "b.rightOuterJoin(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, ('orange', None)),\n",
       " (4, ('blue', 'grey')),\n",
       " (5, ('green', 'black')),\n",
       " (5, ('green', 'white'))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fullOuterJoin\n",
    "b.fullOuterJoin(d).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The reduceByKey operation\n",
    "The reduceByKey operation merges the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer and producing hash-partitioned output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 'black'), (4, 'blue'), (5, 'white'), (5, 'green'), (4, 'grey')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([\"black\", \"blue\", \"white\", \"green\", \"grey\"], 2)\n",
    "b = a.map(lambda x: (len(x), x))\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'bluegrey'), (5, 'blackwhitegreen')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.reduceByKey(lambda x, y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'blue'), (6, 'orange'), (5, 'blackwhite')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([\"black\", \"blue\", \"white\", \"orange\"], 2)\n",
    "b = a.map(lambda x: (len(x), x))\n",
    "b.reduceByKey(lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The aggregate operation\n",
    "The aggregrate operation returns an RDD with the keys of each tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = sc.parallelize([1, 2, 7, 4, 30, 6], 2)\n",
    "z.aggregate(0,(lambda x, y: max(x, y)), (lambda x, y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = sc.parallelize(['a', 'b', 'c', 'd'], 2)\n",
    "z.aggregate('', (lambda x, y: x + y), (lambda x, y: x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssabscd'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.aggregate('s', (lambda x, y: x + y), (lambda x, y: x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'53'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = sc.parallelize([\"12\",\"234\",\"345\",\"56789\"],2)\n",
    "z.aggregate('', (lambda x, y: str(max(len(str(x)), len(str(y))))), (lambda x,y: str(y) + str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.aggregate('', \\\n",
    "            (lambda x, y: str(min(len(str(x)), len(str(y))))), \\\n",
    "            (lambda x,y: str(y) + str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = sc.parallelize([\"12\",\"234\",\"345\",\"\"],2)\n",
    "z.aggregate(\"\",(lambda x, y: str(min(len(str(x)), len(str(y))))),(lambda x,\n",
    "y: str(y) + str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n",
    "Once an RDD has been created, the various transformations get executed only when an action is performed on it. The result of an action can either be data written back to the storage system or returned to the driver program that initiated this for further computation locally to produce the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The collect() function\n",
    "The `collect()` function returns all the results of an RDD operation as an array to the driver program. This is usually useful for operations that produce sufficiently small datasets. Ideally, the result should easily fit in the memory of the system that's hosting the driver program.\n",
    "## The count() function\n",
    "This returns the number of elements in a dataset or the resulting output of an RDD operation.\n",
    "## The take(n) function\n",
    "The `take(n)` function returns the `first (n)` elements of a dataset or the resulting output of an RDD operation.\n",
    "## The first() function\n",
    "The `first()` function returns the first element of the dataset or the resulting output of an RDD operation. It works similarly to the `take(1)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([2, 3, 4]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([2, 3, 4]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([2, 3, 4]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([2, 3, 4]).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The takeSample() function\n",
    "The `takeSample(withReplacement, num, [seed])` function returns an array with a random sample of elements from a dataset. It has three arguments as follows:\n",
    "<ul><li>`withReplacement/withoutReplacement`: This indicates sampling with or without replacement (while taking multiple samples, it indicates whether to replace the old sample back to the set and then take a fresh sample or sample without replacing). For withReplacement, argument should be True and False otherwise.</li>\n",
    "<li>`num`: This indicates the number of elements in the sample.</li>\n",
    "<li>`Seed`: This is a random number generator seed (optional).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 1, 7, 9, 2, 6, 4, 6, 9, 4, 4, 3, 1, 1, 4, 4, 6, 9, 8]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,10))\n",
    "rdd.takeSample(True, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 5, 4, 1, 9, 2, 3]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(False, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 5]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(False, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 6, 5, 5]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(True, 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The countByKey() function\n",
    "The `countByKey()` function is available only on RDDs of type <strong>key value</strong>. It returns a table of <strong>(K, Int)</strong> pairs with the count of each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 2), ('b', 1)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByKey().items()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
