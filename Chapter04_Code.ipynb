{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Spark_Version:2.0.0<br/>\n",
    "Python_Version:Python 3.5.2 | Anaconda4.1.1(64-bit)<br/>\n",
    "Jupyter_Version:4.2.1<br/>\n",
    "System:Ubuntu 16.04 LTS(64-bit)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark_Version: 2.0.0\n",
      "Python_Version: 3.5.2\n",
      "System: Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(\"Spark_Version:\",sc.version)\n",
    "print(\"Python_Version:\",platform.python_version())\n",
    "print(\"System:\",platform.system())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL operations\n",
    "SQL operations are most widely used constructs for data manipulation. Some of most used operations are, selecting all or some of the columns, filtering based on one or more conditions, sorting and grouping operations, and computing summary functions such as average on GroupedData. The JOIN operations on multiple data sources and set\n",
    "operations such as union, intersect and minus are some other operations that are widely performed. Furthermore, data frames are registered as temporary tables and passed traditional SQL statements to perform the aforementioned operations. User-Defined Functions (UDF) are defined and used with and without registration. We'll be focusing on\n",
    "<strong>window</strong> operations, which have been just introduced in Spark 2.0. They address sliding window operations. For example, if you want to report the average peak temperature every day in the past seven days, then you are operating on a sliding window of seven days until today. Here is an example that computes average sales per month for the past three months.The data file contains 24 observations showing monthly sales for two products, P1 and P2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "    <dt>\n",
    "    <tt >rangeBetween</tt>\n",
    "    <big>(</big><em>start</em>, <em>end</em><big>)</big>\n",
    "    </dt>\n",
    "     <dd><p>Defines the frame boundaries, from <cite>start</cite> (inclusive) to <cite>end</cite> (inclusive).</p>\n",
    "        <font color=red><p>Both <cite>start</cite> and <cite>end</cite> are relative from the current row. For example,“0” means “current row”, while “-1” means one off before the current row,and “5” means the five off after the current row.</p></font>\n",
    "        <table>\n",
    "            <tbody>\n",
    "            <tr><th>Parameters:</th><td><ul>\n",
    "            <li><strong>start</strong> – boundary start, inclusive.The frame is unbounded if this is <tt>-sys.maxsize</tt> (or lower).</li>\n",
    "            <li><strong>end</strong> – boundary end, inclusive.The frame is unbounded if this is <tt>sys.maxsize</tt> (or higher).</li>\n",
    "            </ul>\n",
    "            </td>\n",
    "            </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>\n",
    "<tt>pyspark.sql.functions.</tt><tt>bround</tt><big>(</big><em>col</em>, <em>scale=0</em><big>)</big></dt>\n",
    "<dd><p>Round the given value to <cite>scale</cite> decimal places using HALF_EVEN rounding mode if <cite>scale</cite> &gt;= 0\n",
    "or at integral part when <cite>scale</cite> &lt; 0.</p>\n",
    "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mf\">2.5</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\"><span class=\"highlighted\">bround</span></span><span class=\"p\">(</span><span class=\"s\">'a'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span>\n",
    "<span class=\"go\">[Row(r=2.0)]</span>\n",
    "</pre>\n",
    "</dd></dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|Product|Month|Sales|\n",
      "+-------+-----+-----+\n",
      "|     P1|    1|   66|\n",
      "|     P1|    2|   24|\n",
      "|     P1|    3|   54|\n",
      "|     P1|    4|    0|\n",
      "|     P1|    5|   56|\n",
      "|     P1|    6|   34|\n",
      "|     P1|    7|   48|\n",
      "|     P1|    8|   46|\n",
      "|     P1|    9|   76|\n",
      "|     P1|   10|   12|\n",
      "|     P1|   11|    8|\n",
      "|     P1|   12|   24|\n",
      "|     P2|    1|   98|\n",
      "|     P2|    2|   16|\n",
      "|     P2|    3|   78|\n",
      "|     P2|    4|   66|\n",
      "|     P2|    5|   14|\n",
      "|     P2|    6|   76|\n",
      "|     P2|    7|   62|\n",
      "|     P2|    8|   92|\n",
      "+-------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "#Create a DataFrame containing monthly sales data for two products\n",
    "file_path = \"resource/MonthlySales.csv\"\n",
    "monthlySales = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "monthlySales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare WindowSpec to create a 3 month sliding window for a product\n",
    "#Negative subscript denotes rows above current row\n",
    "w = Window\\\n",
    "    .partitionBy(monthlySales[\"Product\"])\\\n",
    "    .orderBy(monthlySales[\"Month\"])\\\n",
    "    .rangeBetween(-2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.window.WindowSpec at 0x7f6dfbfd05c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = func.avg(monthlySales[\"Sales\"]).over(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'avg(Sales) OVER (PARTITION BY Product ORDER BY Month ASC RANGE BETWEEN 2 PRECEDING AND CURRENT ROW)'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+---------+\n",
      "|Product|Sales|Month|MovingAvg|\n",
      "+-------+-----+-----+---------+\n",
      "|     P1|   66|    1|     66.0|\n",
      "|     P1|   24|    2|     45.0|\n",
      "|     P1|   54|    3|     48.0|\n",
      "|     P1|    0|    4|     26.0|\n",
      "|     P1|   56|    5|    36.67|\n",
      "|     P1|   34|    6|     30.0|\n",
      "+-------+-----+-----+---------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monthlySales\\\n",
    ".select(monthlySales.Product,monthlySales.Sales,monthlySales.Month,func.bround(f,2).alias(\"MovingAvg\"))\\\n",
    ".orderBy(monthlySales.Product,monthlySales.Month)\\\n",
    ".show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming\n",
    "Let us look at a simple example. The following example listens to System Activity Report(sar) on Linux on a local machine and computes the average free memory. System Activity Report gives system activity statistics and the current example collects memory usage,reported 20 times at a 2-second interval. The Spark stream reads this streaming output and computes average memory. We use a handy networking utility netcat (nc) to redirect the sar output onto a given port. The options l and k specify that nc should listen for an incoming connection and it has to keep listening for another connection even after its current connection is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "//Run the following command from one terminal window\n",
    "sar -r 2 20 | nc -lk 9999\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In another window, open pyspark shell and do the following\n",
    "myStream = spark.readStream.format(\"socket\").\\\n",
    "                option(\"host\", \"localhost\").\\\n",
    "                option(\"port\", 9999).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter out unwanted lines and then extract free memory part as a float\n",
    "#Drop missing values, if any\n",
    "myDF = myStream.filter(\"value rlike 'IST'\").\\\n",
    "                    select(func.substring(\"value\", 15, 9).cast(\"float\").\\\n",
    "                          alias(\"memFree\")).na.drop().select(\"memFree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avgMemFree = myDF.select(func.avg(\"memFree\"))\n",
    "query = avgMemFree.writeStream. \\\n",
    "            outputMode(\"complete\"). \\\n",
    "                format(\"console\"). \\\n",
    "                        start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+------------+ \n",
    "|avg(memFree)|\n",
    "+------------+\n",
    "|        null|\n",
    "+------------+\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
