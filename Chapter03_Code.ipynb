{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Spark_Version:2.0.0<br/>\n",
    "Python_Version:Python 3.5.2 | Anaconda4.1.1(64-bit)<br/>\n",
    "Jupyter_Version:4.2.1</br>\n",
    "System:Ubuntu 16.04 LTS(64-bit)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark_Version: 2.0.0\n",
      "Python_Version: 3.5.2\n",
      "System: Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(\"Spark_Version:\",sc.version)\n",
    "print(\"Python_Version:\",platform.python_version())\n",
    "print(\"System:\",platform.system())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames from RDDs\n",
    "The following code creates an RDD from a list of colors followed by a collection of tuples containing the color name and its length. It creates a DataFrame using the `toDF` method to convert the RDD into a DataFrame. The `toDF` method takes a list of column labels as an optional argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a list of colours\n",
    "colors = ['white', 'green', 'yellow', 'red', 'brown', 'pink']\n",
    "#Distribute a local collection to form an RDD\n",
    "#Apply map function on that RDD to get another RDD containing colour,length tuples\n",
    "color_df = sc.parallelize(colors).map(lambda x: (x,len(x))).toDF(['color', 'length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[color: string, length: bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('color', 'string'), ('length', 'bigint')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| color|length|\n",
      "+------+------+\n",
      "| white|     5|\n",
      "| green|     5|\n",
      "|yellow|     6|\n",
      "|   red|     3|\n",
      "| brown|     5|\n",
      "|  pink|     4|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "color_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames from JSON\n",
    "JavaScript Object Notation, or JSON, is a language-independent, self-describing, lightweight data-exchange format. `JSON` has become a popular data exchange format and has become ubiquitous. In addition to `JavaScript` and RESTful interfaces, databases such as `MySQL` have accepted `JSON` as a data type and `MongoDB` stores all data as `JSON` documents in binary form. Conversion of data to and from `JSON` is essential for any modern data analysis workflow. The Spark DataFrame API lets developers convert `JSON` objects into DataFrames and vice versa. Let's have a close look at the following examples for a better understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      Mark|    Twain|\n",
      "|   Charles|  Dickens|\n",
      "|    Thomas|    Hardy|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pass the source json data file path\n",
    "df = sqlContext.read.json(\"resource/authors.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames from databases using JDBC\n",
    "Spark allows developers to create DataFrames from other databases using `JDBC`, provided you ensure that the JDBC driver for the intended database is accessible. A `JDBC` driver is a software component that allows a Java application to interact with a database. Different databases require different drivers. Usually, database providers such as `MySQL` supply these driver components to access their databases. You have to ensure that you have the right driver for the database you want to work with.<br/><br/>\n",
    "\n",
    "The following example assumes that you already have a `MySQL` database running at the given URL, a table called people in the database called test with some data in it, and valid credentials to log in. There is an additional step of relaunching the `REPL` shell with the appropriate `JAR` file:<br/><br/>\n",
    "\n",
    "``pyspark --driver-class-path /usr/mySofter/mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar``<br/><br/>\n",
    "\n",
    "If you do not already have the JAR file in your system, download it from the MySQL site at the following link:<br/><br/>\n",
    "https://dev.mysql.com/downloads/connector/j/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "characterSetDF = sqlContext.read.format(\"jdbc\").options(\n",
    "                            url = \"jdbc:mysql://localhost\",\n",
    "                            dbtable = 'information_schema.CHARACTER_SETS',\n",
    "                            user = 'root',\n",
    "                            password = '2038793').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+---------------------------+------+\n",
      "|CHARACTER_SET_NAME|DEFAULT_COLLATE_NAME|DESCRIPTION                |MAXLEN|\n",
      "+------------------+--------------------+---------------------------+------+\n",
      "|big5              |big5_chinese_ci     |Big5 Traditional Chinese   |2     |\n",
      "|dec8              |dec8_swedish_ci     |DEC West European          |1     |\n",
      "|cp850             |cp850_general_ci    |DOS West European          |1     |\n",
      "|hp8               |hp8_english_ci      |HP West European           |1     |\n",
      "|koi8r             |koi8r_general_ci    |KOI8-R Relcom Russian      |1     |\n",
      "|latin1            |latin1_swedish_ci   |cp1252 West European       |1     |\n",
      "|latin2            |latin2_general_ci   |ISO 8859-2 Central European|1     |\n",
      "|swe7              |swe7_swedish_ci     |7bit Swedish               |1     |\n",
      "|ascii             |ascii_general_ci    |US ASCII                   |1     |\n",
      "|ujis              |ujis_japanese_ci    |EUC-JP Japanese            |3     |\n",
      "|sjis              |sjis_japanese_ci    |Shift-JIS Japanese         |2     |\n",
      "|hebrew            |hebrew_general_ci   |ISO 8859-8 Hebrew          |1     |\n",
      "|tis620            |tis620_thai_ci      |TIS620 Thai                |1     |\n",
      "|euckr             |euckr_korean_ci     |EUC-KR Korean              |2     |\n",
      "|koi8u             |koi8u_general_ci    |KOI8-U Ukrainian           |1     |\n",
      "|gb2312            |gb2312_chinese_ci   |GB2312 Simplified Chinese  |2     |\n",
      "|greek             |greek_general_ci    |ISO 8859-7 Greek           |1     |\n",
      "|cp1250            |cp1250_general_ci   |Windows Central European   |1     |\n",
      "|gbk               |gbk_chinese_ci      |GBK Simplified Chinese     |2     |\n",
      "|latin5            |latin5_turkish_ci   |ISO 8859-9 Turkish         |1     |\n",
      "+------------------+--------------------+---------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "characterSetDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames from Apache Parquet\n",
    "Apache Parquet is an efficient, compressed columnar data representation available to any project in the Hadoop ecosystem. Columnar data representations store data by column, as opposed to the traditional approach of storing data row by row. Use cases that require frequent querying of two to three columns from several columns benefit greatly from such an arrangement because columns are stored contiguously on the disk and you do not have to read unwanted columns in row-oriented storage. Another advantage is in compression. Data in a single column belongs to a single type. The values tend to be similar, and sometimes identical. These qualities greatly enhance compression and encoding efficiency. Parquet allows compression schemes to be specified on a per-column level and allows adding more encodings as they are invented and implemented.<br/><br/>\n",
    "Apache Spark provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. The following example writes the people data loaded into a DataFrame in the previous example into Parquet format and then re-reads it into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+---------------------------+------+\n",
      "|CHARACTER_SET_NAME|DEFAULT_COLLATE_NAME|DESCRIPTION                |MAXLEN|\n",
      "+------------------+--------------------+---------------------------+------+\n",
      "|big5              |big5_chinese_ci     |Big5 Traditional Chinese   |2     |\n",
      "|dec8              |dec8_swedish_ci     |DEC West European          |1     |\n",
      "|cp850             |cp850_general_ci    |DOS West European          |1     |\n",
      "|hp8               |hp8_english_ci      |HP West European           |1     |\n",
      "|koi8r             |koi8r_general_ci    |KOI8-R Relcom Russian      |1     |\n",
      "|latin1            |latin1_swedish_ci   |cp1252 West European       |1     |\n",
      "|latin2            |latin2_general_ci   |ISO 8859-2 Central European|1     |\n",
      "|swe7              |swe7_swedish_ci     |7bit Swedish               |1     |\n",
      "|ascii             |ascii_general_ci    |US ASCII                   |1     |\n",
      "|ujis              |ujis_japanese_ci    |EUC-JP Japanese            |3     |\n",
      "|sjis              |sjis_japanese_ci    |Shift-JIS Japanese         |2     |\n",
      "|hebrew            |hebrew_general_ci   |ISO 8859-8 Hebrew          |1     |\n",
      "|tis620            |tis620_thai_ci      |TIS620 Thai                |1     |\n",
      "|euckr             |euckr_korean_ci     |EUC-KR Korean              |2     |\n",
      "|koi8u             |koi8u_general_ci    |KOI8-U Ukrainian           |1     |\n",
      "|gb2312            |gb2312_chinese_ci   |GB2312 Simplified Chinese  |2     |\n",
      "|greek             |greek_general_ci    |ISO 8859-7 Greek           |1     |\n",
      "|cp1250            |cp1250_general_ci   |Windows Central European   |1     |\n",
      "|gbk               |gbk_chinese_ci      |GBK Simplified Chinese     |2     |\n",
      "|latin5            |latin5_turkish_ci   |ISO 8859-9 Turkish         |1     |\n",
      "+------------------+--------------------+---------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "try :\n",
    "    shutil.rmtree('outputs')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "#Write DataFrame contents into Parquet format\n",
    "characterSetDF.write.parquet('outputs/writers.parquet')\n",
    "#Read Parquet data into another DataFrame\n",
    "writersDF = sqlContext.read.parquet(\"outputs/writers.parquet\")\n",
    "\n",
    "writersDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames from other data sources\n",
    "Spark provides built-in support for multiple data sources such as `JSON`, `JDBC`, `HDFS`,`Parquet`, `MYSQL`, `Amazon S3`, and so on. In addition, it provides a Data Source API that provides a pluggable mechanism for accessing structured data through Spark SQL. There are several libraries built on top of this pluggable component, for example, `CSV`, `Avro`, `Cassandra`, and `MongoDB`, to name a few. These libraries are not part of the Spark code base. These are built for individual data sources and hosted on a community site, Spark packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame operations\n",
    "In the previous section of this chapter, we learnt many different ways of creating DataFrames. In this section, we will focus on various operations that can be performed on DataFrames. Developers chain multiple operations to `filter`, `transform`, `aggregate`, and `sort data` in the DataFrames. The underlying Catalyst optimizer ensures efficient execution of these operations. These functions you find here are similar to those you commonly find in `SQL` operations on tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| color|length|\n",
      "+------+------+\n",
      "| white|     5|\n",
      "| green|     5|\n",
      "|yellow|     6|\n",
      "|   red|     3|\n",
      "| brown|     5|\n",
      "|  pink|     4|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a local collection of colors first\n",
    "colors = ['white', 'green', 'yellow', 'red', 'brown', 'pink']\n",
    "#Distribute the local collection to form an RDD length tuples and convert that RDD to a DataFrame\n",
    "color_df = sc.parallelize(colors).map(lambda x:(x,len(x))).toDF(['color', 'length'])\n",
    "color_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('color', 'string'), ('length', 'bigint')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['color', 'length']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| color|\n",
      "+------+\n",
      "| white|\n",
      "| green|\n",
      "|yellow|\n",
      "|   red|\n",
      "| brown|\n",
      "|  pink|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop a column. The source DataFrame color_df remains the same. \n",
    "#Spark returns a new DataFrame which is being passed to show\n",
    "color_df.drop('length').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"color\":\"white\",\"length\":5}',\n",
       " '{\"color\":\"green\",\"length\":5}',\n",
       " '{\"color\":\"yellow\",\"length\":6}',\n",
       " '{\"color\":\"red\",\"length\":3}',\n",
       " '{\"color\":\"brown\",\"length\":5}',\n",
       " '{\"color\":\"pink\",\"length\":4}']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert to JSON format\n",
    "color_df.toJSON().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|mid_length|\n",
      "+----------+\n",
      "|     white|\n",
      "|     green|\n",
      "|     brown|\n",
      "|      pink|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter operation is similar to WHERE clause in SQL\n",
    "#You specify conditions to select only desired columns and rows\n",
    "\n",
    "#Output of filter operation is another DataFrame object that is usually passed on to some more operations\n",
    "#The following example selects the colors having a length of four or five only and label the column as \"mid_length\" \n",
    "color_df.filter(color_df.length.between(4, 5)).select(color_df.color.alias(\"mid_length\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| color|length|\n",
      "+------+------+\n",
      "| green|     5|\n",
      "|yellow|     6|\n",
      "| brown|     5|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This example uses multiple filter criteria\n",
    "color_df.filter(color_df.length > 4)\\\n",
    "        .filter(color_df[0] != \"white\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| color|length|\n",
      "+------+------+\n",
      "| brown|     5|\n",
      "| green|     5|\n",
      "|  pink|     4|\n",
      "|   red|     3|\n",
      "| white|     5|\n",
      "|yellow|     6|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sort the data on one or more columns\n",
    "color_df.sort(\"color\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| color|length|\n",
      "+------+------+\n",
      "|yellow|     6|\n",
      "| white|     5|\n",
      "| green|     5|\n",
      "| brown|     5|\n",
      "|  pink|     4|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#First filter colors of length more than 4 and then sort on multiple columns\n",
    "#The Filtered rows are sorted first on the column length in default ascending order\n",
    "#Rows with same length are sorted on color in descending\n",
    "color_df.filter(color_df['length'] >= 4).sort(\"length\", \"color\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(color='red', length=3),\n",
       " Row(color='pink', length=4),\n",
       " Row(color='brown', length=5),\n",
       " Row(color='green', length=5)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can use orderBy instead, which is an alias to sort\n",
    "color_df.orderBy(\"length\", \"color\").take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| color|length|\n",
      "+------+------+\n",
      "|yellow|     6|\n",
      "| brown|     5|\n",
      "| green|     5|\n",
      "| white|     5|\n",
      "|  pink|     4|\n",
      "|   red|     3|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Alternative syntax, for single or multiple columns.\n",
    "color_df.sort(color_df.length.desc(), color_df.color.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     6|    1|\n",
      "|     5|    3|\n",
      "|     3|    1|\n",
      "|     4|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#All the examples until now have been acting on one row at a time,\n",
    "#filtering or transforming or reordering.Introduction to DataFrames\n",
    "\n",
    "#The following example deals with regrouping the data\n",
    "#These operations require \"wide dependency\" and often involve shuffling.\n",
    "\n",
    "color_df.groupBy(\"length\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      Mark|    Twain|\n",
      "|   Charles|  Dickens|\n",
      "|      null|    Hardy|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data often contains missing information or null values. \n",
    "#We may want to drop such rows or replace with some filler information. \n",
    "#dropna is provided for dropping such rows\n",
    "#The following json file has names of famous authors. Firstname data is missing in one row.\n",
    "df1 = sqlContext.read.json('resource/authors_missing.json')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      Mark|    Twain|\n",
      "|   Charles|  Dickens|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let us drop the row with incomplete information\n",
    "df2 = df1.dropna()\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
