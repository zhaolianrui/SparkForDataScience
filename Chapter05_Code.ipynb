{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Spark_Version:2.0.1<br/>\n",
    "Python_Version:Python 3.5.2 | Anaconda4.2.0(64-bit)<br/>\n",
    "Jupyter_Version:4.2.3  &emsp;Kernel:Python[default]<br/>\n",
    "System:Ubuntu 16.04 LTS(64-bit)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark_Version: 2.0.1\n",
      "Python_Version: 3.5.2\n",
      "System: Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(\"Spark_Version:\",sc.version)\n",
    "print(\"Python_Version:\",platform.python_version())\n",
    "print(\"System:\",platform.system())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "<p><strong>Data acquisition</strong>, or <strong>data collection</strong>, is the very first step in any data science project. <font color=red>Usually,you won't find the complete set of required data in one place as it is distributed across lineof-business (LOB) applications and systems.</font></p>\n",
    "<p>The majority of this section has already been covered in the previous chapter, which outlined how to source data from different data sources and store the data in DataFrames for easier analysis. There is a built-in mechanism in Spark to fetch data from some of the common data sources and the Data Source API is provided for the ones not supported out of the box on Spark.</p><p>To get a better understanding of the data acquisition and preparation phases, let us assume a scenario and try to address all the steps involved with example code snippets. The scenario is such that employee data is present across native RDDs, JSON files, and on a SQL server. So, let's see how we can get those to Spark DataFrames:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#From RDD: Create an RDD and convert to DataFrame\n",
    "employees = sc.parallelize([(1, 'John', 25), (2, 'Ray', 35), (3, 'Mike', 24), \n",
    "                            (4, 'Jane', 28), (5, 'Kevin', 26), (6, 'Vincent', 35), \n",
    "                            (7, 'James', 38), (8, 'Shane', 32), (9, 'Larry', 29), \n",
    "                            (10, 'Kimberly', 29), (11, 'Alex', 28), (12, 'Garry', 25), \n",
    "                            (13, 'Max', 31)]).toDF(['emp_id', 'name', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+\n",
      "|emp_id|    name|age|\n",
      "+------+--------+---+\n",
      "|     1|    John| 25|\n",
      "|     2|     Ray| 35|\n",
      "|     3|    Mike| 24|\n",
      "|     4|    Jane| 28|\n",
      "|     5|   Kevin| 26|\n",
      "|     6| Vincent| 35|\n",
      "|     7|   James| 38|\n",
      "|     8|   Shane| 32|\n",
      "|     9|   Larry| 29|\n",
      "|    10|Kimberly| 29|\n",
      "|    11|    Alex| 28|\n",
      "|    12|   Garry| 25|\n",
      "|    13|     Max| 31|\n",
      "+------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From JSON: reading a JSON file\n",
    "salary = sqlContext.read.json(\"resource/salary.json\")\n",
    "designation = sqlContext.read.json(\"resource/designation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|e_id|salary|\n",
      "+----+------+\n",
      "|   1| 10000|\n",
      "|   2| 12000|\n",
      "|   3| 12000|\n",
      "|   4|  null|\n",
      "|   5|   120|\n",
      "|   6| 22000|\n",
      "|   7| 20000|\n",
      "|   8| 12000|\n",
      "|   9| 10000|\n",
      "|  10|  8000|\n",
      "|  11| 12000|\n",
      "|  12| 12000|\n",
      "|  13|120000|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|          role|\n",
      "+---+--------------+\n",
      "|  1|     Associate|\n",
      "|  2|       Manager|\n",
      "|  3|       Manager|\n",
      "|  4|     Associate|\n",
      "|  5|       Manager|\n",
      "|  6|Senior Manager|\n",
      "|  7|Senior Manager|\n",
      "|  8|       Manager|\n",
      "|  9|       Manager|\n",
      "| 10|     Associate|\n",
      "| 11|       Manager|\n",
      "| 12|       Manager|\n",
      "| 13|       Manager|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "designation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "<p>Data quality has always been a pervasive problem in the industry. The presence of incorrect or inconsistent data can produce misleading results of your analysis. Implementing better algorithm or building better models will not help much if the data is not cleansed and prepared well, as per the requirement. There is an industry jargon called <strong>data engineering</strong> that refers to <strong>data sourcing</strong> and <strong>preparation</strong>. This is typically done by data scientists and in a few organizations, there is a dedicated team for this purpose. However, <font color=red>while preparing data, a scientific perspective is often needed to do it right. As an example, you may not just do mean substitution to treat missing values and look into data distribution to find more appropriate values to substitute. Another such example is that you may not just look at a box plot or scatter plot to look for outliers, as there could be multivariate outliers which are not visible if you plot a single variable.</font> There are different approaches, such as <strong>Gaussian Mixture Models (GMMs)</strong> and <strong>Expectation Maximization (EM)</strong> algorithms that use <strong>Mahalanobis distance</strong> to look for multivariate outliers.</p>\n",
    "<p>The data preparation phase is an extremely important phase, not only for the algorithms to work properly, but also for you to develop a better understanding of your data so that you can take the right approach while implementing an algorithm.</p>\n",
    "<p>Once the data has been acquired from different sources, the next step is to consolidate them all so that the data as a whole can be cleaned, formatted, and transformed to the format needed for your analysis. Please note that you might have to take samples of data from the sources, depending on the scenario, and then prepare the data for further analysis. Various sampling techniques that can be used are discussed later in this chapter.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data consolidation\n",
    "In this section, we will take a look at how to combine data acquired from various data sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+---------+------+\n",
      "|emp_id| name|age|     role|salary|\n",
      "+------+-----+---+---------+------+\n",
      "|     1| John| 25|Associate| 10000|\n",
      "|     2|  Ray| 35|  Manager| 12000|\n",
      "|     3| Mike| 24|  Manager| 12000|\n",
      "|     4| Jane| 28|Associate|  null|\n",
      "|     5|Kevin| 26|  Manager|   120|\n",
      "+------+-----+---+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating the final data matrix using the join operation\n",
    "final_data = employees.join(salary, employees.emp_id == salary.e_id).\\\n",
    "                       join(designation, employees.emp_id == designation.id).\\\n",
    "                       select(\"emp_id\", \"name\", \"age\", \"role\", \"salary\")\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleansing\n",
    "<p>Once you have the data consolidated in one place, it is extremely important that you spend enough time and effort in cleaning it before analyzing it. This is an iterative process because you have to validate the actions you have taken on the data and continue till you are satisfied with the data quality. It is advisable that you spend time analyzing the causes of anomalies you detect in the data.</p>\n",
    "<p>Some level of impurity in data usually exists in any dataset. There can be various kinds of issues with data, but we are going to address a few common cases, such as <strong>missing values</strong>,<strong>duplicate values</strong>, <strong>transforming</strong>, or <strong>formatting </strong>(adding or removing digits from a number,splitting a column into two, merging two columns into one).</p>\n",
    "\n",
    "## Missing value treatment\n",
    "\n",
    "<p>There are various ways of handling missing values. One way is dropping rows containing missing values. We may want to drop a row even if a single column has missing value, or may have different strategies for different columns. We may want to retain the row as long as the total number of missing values in that row are under a threshold. Another approach may be to replace nulls with a constant value, say the mean value in case of numeric\n",
    "variables.</p>\n",
    "<p>In this section, we will not be providing some examples in both Scala and Python and will try to cover various scenarios to give you a broader perspective.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+--------------+------+\n",
      "|emp_id|    name|age|          role|salary|\n",
      "+------+--------+---+--------------+------+\n",
      "|     1|    John| 25|     Associate| 10000|\n",
      "|     2|     Ray| 35|       Manager| 12000|\n",
      "|     3|    Mike| 24|       Manager| 12000|\n",
      "|     5|   Kevin| 26|       Manager|   120|\n",
      "|     6| Vincent| 35|Senior Manager| 22000|\n",
      "|     7|   James| 38|Senior Manager| 20000|\n",
      "|     8|   Shane| 32|       Manager| 12000|\n",
      "|     9|   Larry| 29|       Manager| 10000|\n",
      "|    10|Kimberly| 29|     Associate|  8000|\n",
      "|    11|    Alex| 28|       Manager| 12000|\n",
      "|    12|   Garry| 25|       Manager| 12000|\n",
      "|    13|     Max| 31|       Manager|120000|\n",
      "+------+--------+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping rows with missing value(s)\n",
    "clean_data = final_data.na.drop()\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replacing missing value by mean\n",
    "import math\n",
    "from pyspark.sql import functions as F\n",
    "mean_salary = math.floor(salary.select(F.mean('salary')).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20843"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+--------------+------+\n",
      "|emp_id|    name|age|          role|salary|\n",
      "+------+--------+---+--------------+------+\n",
      "|     1|    John| 25|     Associate| 10000|\n",
      "|     2|     Ray| 35|       Manager| 12000|\n",
      "|     3|    Mike| 24|       Manager| 12000|\n",
      "|     4|    Jane| 28|     Associate| 20843|\n",
      "|     5|   Kevin| 26|       Manager|   120|\n",
      "|     6| Vincent| 35|Senior Manager| 22000|\n",
      "|     7|   James| 38|Senior Manager| 20000|\n",
      "|     8|   Shane| 32|       Manager| 12000|\n",
      "|     9|   Larry| 29|       Manager| 10000|\n",
      "|    10|Kimberly| 29|     Associate|  8000|\n",
      "|    11|    Alex| 28|       Manager| 12000|\n",
      "|    12|   Garry| 25|       Manager| 12000|\n",
      "|    13|     Max| 31|       Manager|120000|\n",
      "+------+--------+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data = final_data.na.fill({'salary' : mean_salary})\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|   Thomas|   Hardy|    June 2, 1840|\n",
      "|  Charles| Dickens| 7 February 1812|\n",
      "|     Mark|   Twain|            null|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|    Emily|    null|            null|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Another example for missing value treatment\n",
    "authors = [['Thomas', 'Hardy', 'June 2, 1840'], \n",
    "           ['Charles', 'Dickens', '7 February 1812'],\n",
    "           ['Mark', 'Twain', None],\n",
    "           ['Jane', 'Austen', '16 December 1775'],\n",
    "           ['Emily', None, None]]\n",
    "df1 = sc.parallelize(authors).toDF(['FirstName', 'LastName', 'Dob'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|   Thomas|   Hardy|    June 2, 1840|\n",
      "|  Charles| Dickens| 7 February 1812|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop rows with missing values\n",
    "df1.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|   Thomas|   Hardy|    June 2, 1840|\n",
      "|  Charles| Dickens| 7 February 1812|\n",
      "|     Mark|   Twain|            null|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop rows with at least 2 missing values\n",
    "df1.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|   Thomas|   Hardy|    June 2, 1840|\n",
      "|  Charles| Dickens| 7 February 1812|\n",
      "|     Mark|   Twain|         Unknown|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|    Emily| Unknown|         Unknown|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fill all missing values with a given string\n",
    "df1.na.fill(\"Unknown\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|   Thomas|   Hardy|    June 2, 1840|\n",
      "|  Charles| Dickens| 7 February 1812|\n",
      "|     Mark|   Twain|         Unknown|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|    Emily|      --|         Unknown|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fill missing values in each column with a given string\n",
    "df1.na.fill({'LastName':'--','Dob':'Unknown'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier treatment\n",
    "\n",
    "Understanding what an outlier is also important to treat it well. To put it simply, an outlier is a data point that does not share the same characteristics as the rest of the data points. Example: if you have a dataset of schoolchildren and there are a few age values in the range of 30-40 then they could be outliers. Let us look into a different example now: if you have a dataset where a variable can have data points only in two ranges, say, in the 10-20 or 80-90 range, then the data points (say, 40 or 55) with values in between these two ranges could also be outliers. In this example, 40 or 55 do not belong to the 10-20 range, nor do they belong to the 80-90 range, and are outliers.Also, there can be **univariate outliers** and there can be **multivariate outliers** as well. We will focus on univariate outliers in this book for simplicity's sake as Spark MLlib may not have all the algorithms needed at the time of writing this book.\n",
    "\n",
    "In order to treat the outliers, you have to first see if there are outliers. There are different ways, such as summary statistics and plotting techniques, to find the outliers. You can use the built-in library functions such as **matplotlib** of Python to visualize your data. You can do so by connecting to Spark through a notebook (for example, **Jupyter**) so that the visuals can be generated, which may not be possible on a command shell. Once you find outliers, you can either delete the rows containing outliers or impute the **mean values** in place of outliers or do something more relevant, as applicable to your case.Let us have a look at the **mean substitution** method here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20843"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify outliers and replace them with mean\n",
    "# The following example reuses the clean_data dataset and mean_salary computed in previous examples\n",
    "mean_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    deviation|\n",
      "+-------------+\n",
      "| 1.17570649E8|\n",
      "|  7.8198649E7|\n",
      "|  7.8198649E7|\n",
      "|         null|\n",
      "| 4.29442729E8|\n",
      "|    1338649.0|\n",
      "|     710649.0|\n",
      "|  7.8198649E7|\n",
      "| 1.17570649E8|\n",
      "| 1.64942649E8|\n",
      "|  7.8198649E7|\n",
      "|  7.8198649E7|\n",
      "|9.832110649E9|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compute deviation for each row\n",
    "devs = final_data.select(((final_data.salary - mean_salary) ** 2).alias(\"deviation\"))\n",
    "devs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30351"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute standard deviation\n",
    "stddev = math.floor(math.sqrt(devs.groupBy().avg(\"deviation\").first()[0]))\n",
    "stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30351"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(stddev, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>\n",
    "<tt>pyspark.sql.functions.</tt><tt>when</tt><big>(</big><em>condition</em>, <em>value</em><big>)</big></dt>\n",
    "<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
    "If <tt>Column.otherwise()</tt> is not invoked, None is returned for unmatched conditions.</p>\n",
    "<table class=\"docutils field-list\" frame=\"void\" rules=\"none\">\n",
    "<col class=\"field-name\" />\n",
    "<col class=\"field-body\" />\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"field-odd field\"><th class=\"field-name\">Parameters:</th><td class=\"field-body\"><ul class=\"first last simple\">\n",
    "<li><strong>condition</strong> &#8211; a boolean <tt class=\"xref py py-class docutils literal\"><span class=\"pre\">Column</span></tt> expression.</li>\n",
    "<li><strong>value</strong> &#8211; a literal value, or a <tt class=\"xref py py-class docutils literal\"><span class=\"pre\">Column</span></tt> expression.</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<pre>\n",
    ">>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
    "[Row(age=3), Row(age=4)]\n",
    "\n",
    ">>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
    "[Row(age=3), Row(age=None)]\n",
    "\n",
    "</pre>\n",
    "\n",
    "</dd></dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace outliers beyond 2 standard deviations with the mean salary\n",
    "no_outlier = final_data.select(final_data.emp_id, final_data.name, \n",
    "                               final_data.age, final_data.salary, final_data.role,\n",
    "                               F.when(final_data.salary.between(mean_salary-(2*stddev),mean_salary+(2*stddev)),\n",
    "                                    final_data.salary).otherwise(mean_salary).alias(\"updated_salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+------+-------+--------------+\n",
      "|emp_id|name|age|salary|   role|updated_salary|\n",
      "+------+----+---+------+-------+--------------+\n",
      "|    13| Max| 31|120000|Manager|         20843|\n",
      "+------+----+---+------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observe modified values\n",
    "no_outlier.filter(no_outlier.salary != no_outlier.updated_salary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|no_outlier_deviation|\n",
      "+--------------------+\n",
      "|        1.17570649E8|\n",
      "|         7.8198649E7|\n",
      "|         7.8198649E7|\n",
      "|                 0.0|\n",
      "|        4.29442729E8|\n",
      "|           1338649.0|\n",
      "|            710649.0|\n",
      "|         7.8198649E7|\n",
      "|        1.17570649E8|\n",
      "|        1.64942649E8|\n",
      "|         7.8198649E7|\n",
      "|         7.8198649E7|\n",
      "|                 0.0|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_outlier_devs = no_outlier.select(((no_outlier.updated_salary - mean_salary) ** 2).alias(\"no_outlier_deviation\"))\n",
    "no_outlier_devs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9697"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_outlier_stddev = math.floor(math.sqrt(no_outlier_devs.groupBy().avg(\"no_outlier_deviation\").first()[0]))\n",
    "no_outlier_stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate values treatment\n",
    "There are different ways of treating the duplicate records in a dataset. We will demonstrate those in the following code snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "|   Thomas|       H|            null|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|    Emily|    null|            null|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Deleting the duplicate rows\n",
    "authors = [['Thomas', 'Hardy', 'June 2,1840'],\n",
    "           ['Thomas', 'Hardy', 'June 2,1840'],\n",
    "           ['Thomas', 'H', None], \n",
    "           ['Jane', 'Austen', '16 December 1775'],\n",
    "           ['Emily', None,None]]\n",
    "df1 = sc.parallelize(authors).toDF([\"FirstName\",\"LastName\",\"Dob\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|    Emily|    null|            null|\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "|   Thomas|       H|            null|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicated rows\n",
    "df1.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|    Emily|    null|            null|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates based on a sub set of columns\n",
    "df1.dropDuplicates(subset=['FirstName']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation\n",
    "\n",
    "There can be various kinds of data transformation needs and every case is mostly unique.We are going to cover some basic types of transformations, as follows:\n",
    "* Merging two columns into one\n",
    "* Adding characters/numbers to the existing ones\n",
    "* Deleting or replacing characters/numbers from the existing ones\n",
    "* Changing date formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "    <dt><tt>withColumn</tt><big>(</big><em>colName</em>, <em>col</em><big>)</big></dt>\n",
    "    <dd>\n",
    "    <p>Returns a new <tt >DataFrame</tt> by adding a column or replacing the existing column that has the same name.</p>\n",
    "\n",
    "<table frame=\"void\" rules=\"none\">\n",
    "<tbody valign=\"top\">\n",
    "<tr><th>Parameters:</th><td><ul>\n",
    "<li><strong>colName</strong> &#8211; string, name of the new column.</li>\n",
    "<li><strong>col</strong> &#8211; a <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=when#pyspark.sql.Column\"><tt>Column</tt></a> expression for the new column.</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<pre>\n",
    ">>> df.withColumn('age2', df.age + 2).collect()\n",
    "[Row(age=2, name=u'Alice', age2=4), Row(age=5, name=u'Bob', age2=7)]\n",
    "</pre>\n",
    "</dd></dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---------+------+--------+\n",
      "|emp_id|name|age|     role|salary|name_age|\n",
      "+------+----+---+---------+------+--------+\n",
      "|     1|John| 25|Associate| 10000| John_25|\n",
      "|     2| Ray| 35|  Manager| 12000|  Ray_35|\n",
      "|     3|Mike| 24|  Manager| 12000| Mike_24|\n",
      "|     4|Jane| 28|Associate|  null| Jane_28|\n",
      "+------+----+---+---------+------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merging columns\n",
    "# Create a udf to concatenate two column values\n",
    "import pyspark.sql.functions\n",
    "concat_func = pyspark.sql.functions.udf(lambda name, age: name + '_' + str(age))\n",
    "# Apply the udf to create merged column\n",
    "concat_df = final_data.withColumn('name_age', concat_func(final_data.name, final_data.age))\n",
    "concat_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---------+------+--------+---------------+\n",
      "|emp_id|name|age|     role|salary|name_age|age_incremented|\n",
      "+------+----+---+---------+------+--------+---------------+\n",
      "|     1|John| 25|Associate| 10000| John_25|             35|\n",
      "|     2| Ray| 35|  Manager| 12000|  Ray_35|             45|\n",
      "|     3|Mike| 24|  Manager| 12000| Mike_24|             34|\n",
      "|     4|Jane| 28|Associate|  null| Jane_28|             38|\n",
      "+------+----+---+---------+------+--------+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding constant to data\n",
    "data_new = concat_df.withColumn('age_incremented', concat_df.age + 10)\n",
    "data_new.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>\n",
    "<tt>replace</tt><big>(</big><em>to_replace</em>, <em>value</em>, <em>subset=None</em><big>)</big></dt>\n",
    "<dd><p>Returns a new <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=replace#pyspark.sql.DataFrame\"><tt>DataFrame</tt></a> replacing a value with another value.\n",
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=replace#pyspark.sql.DataFrame.replace\"><tt>DataFrame.replace()</tt></a> and <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=replace#pyspark.sql.DataFrameNaFunctions.replace\"><tt>DataFrameNaFunctions.replace()</tt></a> are\n",
    "aliases of each other.</p>\n",
    "<table frame=\"void\" rules=\"none\">\n",
    "\n",
    "<tbody valign=\"top\">\n",
    "<tr><th>Parameters:</th><td><ul>\n",
    "<li><strong>to_replace</strong> &#8211; int, long, float, string, or list.<strong>Value to be replaced</strong>.If the value is a dict, then <cite>value</cite> is ignored and <cite>to_replace</cite> must be a mapping from column name (string) to replacement value. The value to be replaced must be an int, long, float, or string.</li>\n",
    "<li><strong>value</strong> &#8211; int, long, float, string, or list.\n",
    "<strong>Value to use to replace holes</strong>.The replacement value must be an int, long, float, or string. If <cite>value</cite> is a list or tuple, <cite>value</cite> should be of the same length with <cite>to_replace</cite>.</li>\n",
    "<li><strong>subset</strong> &#8211; optional list of column names to consider.Columns specified in subset that do not have matching data type are ignored.For example, if <cite>value</cite> is a string, and subset contains a non-string column,then the non-string column is simply ignored.</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<pre>\n",
    ">>> df4.na.replace(10, 20).show()\n",
    "+----+------+-----+\n",
    "| age|height| name|\n",
    "+----+------+-----+\n",
    "|  20|    80|Alice|\n",
    "|   5|  null|  Bob|\n",
    "|null|  null|  Tom|\n",
    "|null|  null| null|\n",
    "+----+------+-----+\n",
    "\n",
    ">>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
    "+----+------+----+\n",
    "| age|height|name|\n",
    "+----+------+----+\n",
    "|  10|    80|   A|\n",
    "|   5|  null|   B|\n",
    "|null|  null| Tom|\n",
    "|null|  null|null|\n",
    "+----+------+----+\n",
    "</pre>\n",
    "</dd></dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "|   Thomas|       H|            null|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|Charlotte|    null|            null|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace values in a column\n",
    "df1.replace(\"Emily\", \"Charlotte\", \"FirstName\").show()\n",
    "#If the column name argument is omitted in replace, then replacement is applicable to all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>\n",
    "<tt>withColumn</tt><big>(</big><em>colName</em>, <em>col</em><big>)</big></dt>\n",
    "<dd><p>Returns a new <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=replace#pyspark.sql.DataFrame\"><tt>DataFrame</tt></a> by adding a column or replacing the existing column that has the same name.</p>\n",
    "<table frame=\"void\" rules=\"none\">\n",
    "<tbody valign=\"top\">\n",
    "<tr><th>Parameters:</th><td><ul>\n",
    "<li><strong>colName</strong> – string, name of the new column.</li>\n",
    "<li><strong>col</strong> – a <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=when#pyspark.sql.Column\"><tt>Column</tt></a> expression for the new column.</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<pre>\n",
    ">>> df.withColumn('age2', df.age + 2).collect()\n",
    "[Row(age=2, name=u'Alice', age2=4), Row(age=5, name=u'Bob', age2=7)]\n",
    "</pre>\n",
    "</dd></dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>\n",
    "<tt>substr</tt><big>(</big><em>startPos</em>, <em>length</em><big>)</big></dt>\n",
    "<dd><p>Return a <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=substr#pyspark.sql.Column\" ><tt>Column</tt></a> which is a substring of the column.</p>\n",
    "<table frame=\"void\" rules=\"none\">\n",
    "\n",
    "<tbody valign=\"top\">\n",
    "<tr><th>Parameters:</th><td><ul>\n",
    "<li><strong>startPos</strong> &#8211; start position (int or Column)</li>\n",
    "<li><strong>length</strong> &#8211; length of the substring (int or Column)</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<pre>\n",
    ">>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
    "[Row(col=u'Ali'), Row(col=u'Bob')]\n",
    "</pre>\n",
    "</dd></dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+-------+\n",
      "|FirstName|LastName|             Dob|Initial|\n",
      "+---------+--------+----------------+-------+\n",
      "|   Thomas|   Hardy|     June 2,1840|      H|\n",
      "|   Thomas|   Hardy|     June 2,1840|      H|\n",
      "|   Thomas|       H|            null|      H|\n",
      "|     Jane|  Austen|16 December 1775|      A|\n",
      "|    Emily|    null|            null|   null|\n",
      "+---------+--------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Append new columns based on existing values in a column\n",
    "#Give 'LastName' instead of 'Initial' if you want to overwrite\n",
    "df1.withColumn('Initial', df1.LastName.substr(1, 1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with basic examples, let us put together a somewhat complex example. You might have noticed that the date column in Authors data has different date formats. In some cases, month is followed by day, and vice versa. Such anomalies are\n",
    "common in the real world, wherein data might be collected from different sources. Here, we are looking at a case where the date column has data points with many different date formats. We need to standardize all the different date formats into one format. To do so, we first have to create a user-defined function (udf) that can take care of the different formats and convert those to one common format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**格式限定符**\n",
    "它有着丰富的的“格式限定符”（语法是{}中带:号），比如：\n",
    "\n",
    "<strong>填充与对齐</strong><br/>\n",
    "填充常跟对齐一起使用,居中^、左对齐<、右对齐>，后面带宽度,:号后面带填充的字符，只能是一个字符，不指定的话默认是用空格填充,比如:<br/>\n",
    "\n",
    "```\n",
    "In [01]: '{:>8}'.format('189')\n",
    "Out[01]: '   189'\n",
    "In [02]: '{:0>8}'.format('189')\n",
    "Out[02]: '00000189'\n",
    "In [03]: '{:a>8}'.format('189')\n",
    "Out[03]: 'aaaaa189'\n",
    "```\n",
    "<strong>精度与类型f</strong><br/>\n",
    "精度常跟类型f一起使用:<br/>\n",
    "```\n",
    "In [04]: '{:.2f}'.format(321.33345)\n",
    "Out[04]: '321.33'\n",
    "```\n",
    "其中.2表示长度为2的精度，f表示float类型。<br/>\n",
    "<strong>其他类型</strong>主要就是进制了，b、d、o、x分别是二进制、十进制、八进制、十六进制:<br/>\n",
    "```\n",
    "In [05]: '{:b}'.format(17)\n",
    "Out[05]: '10001'\n",
    "In [06]: '{:d}'.format(17)\n",
    "Out[06]: '17'\n",
    "In [07]: '{:o}'.format(17)\n",
    "Out[07]: '21'\n",
    "In [08]: '{:x}'.format(17)\n",
    "Out[08]: '11'\n",
    "```\n",
    "用，还能用来做金额的千位分隔符:<br/>\n",
    "```\n",
    "In [08]: '{:,}'.format(1234567890)\n",
    "Out[08]: '1,234,567,890'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+\n",
      "|FirstName|LastName|       Dob|\n",
      "+---------+--------+----------+\n",
      "|   Thomas|   Hardy|1840-06-02|\n",
      "|  Charles| Dickens|1812-02-07|\n",
      "|     Mark|   Twain|      null|\n",
      "|     Jane|  Austen|1775-12-16|\n",
      "|    Emily|    null|      null|\n",
      "+---------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Date conversions\n",
    "#Create udf for date conversion that converts incoming string to YYYY-MMDD format\n",
    "#The function assumes month is full month name and year is always 4 digits\n",
    "#Separator is always a space or comma\n",
    "#Month, date and year may come in any order\n",
    "#Reusing authors data\n",
    "\n",
    "authors = [['Thomas', 'Hardy', 'June 2, 1840'], \n",
    "          ['Charles', 'Dickens', '7 February 1812'],\n",
    "          ['Mark', 'Twain', None], \n",
    "          ['Jane', 'Austen', '16 December 1775'], \n",
    "          ['Emily', None, None]]\n",
    "\n",
    "df1 = sc.parallelize(authors).toDF(['FirstName', 'LastName', 'Dob'])\n",
    "\n",
    "#define udf\n",
    "#Note:You may create this in a script file and execute with execfile(filename.py)\n",
    "def toDate(s):\n",
    "    import re\n",
    "    year = month = day = \"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    mn = [0,'January', 'February', 'March', \n",
    "          'April', 'May', 'June', 'July', \n",
    "          'August', 'September', 'October', 'November', 'December']\n",
    "    #Split the string and remove empty tokens\n",
    "    l = [tok for tok in re.split(',| ',s) if tok]\n",
    "    #Assign token to year ,month or day\n",
    "    for a in l:\n",
    "        if a in mn:\n",
    "            month = '{:0>2d}'.format(mn.index(a))\n",
    "            print(month)\n",
    "        elif len(a) == 4:\n",
    "            year = a\n",
    "        elif len(a) == 1:\n",
    "            day = '0' + a\n",
    "        else:\n",
    "            day = a\n",
    "    return year + '-' + month + '-' + day\n",
    "\n",
    "#Register the udf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "toDateUDF = udf(toDate, StringType())\n",
    "\n",
    "#Apply udf\n",
    "df1.withColumn(\"Dob\", toDateUDF(\"Dob\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
